{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810899a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from test_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac6212a",
   "metadata": {},
   "source": [
    "# Vanilla Version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea30f76",
   "metadata": {},
   "source": [
    "## GRU\n",
    "<center>\n",
    "<img src=\"assets/GRUV.png\" width=\"80%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffe12deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Cell(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim):\n",
    "        super(GRU_Cell, self).__init__()\n",
    "        \n",
    "        self.linear_r = nn.Linear(input_dim + hidden_dim, hidden_dim) # reset gate \n",
    " \n",
    "        self.linear_z = nn.Linear(input_dim + hidden_dim, hidden_dim) # update gate \n",
    "                \n",
    "        self.linear_h = nn.Linear(input_dim + hidden_dim, hidden_dim) # hidden candidate state\n",
    "        \n",
    "        # input_dim + hidden_dim , because it takes the combine (1,2)\n",
    "        \n",
    "    def forward(self,x_t, h_prev):        \n",
    "\n",
    "        combined = torch.cat((x_t,h_prev),dim=1)\n",
    "        \n",
    "        r_t = torch.sigmoid(self.linear_r(combined))\n",
    "        \n",
    "        z_t = torch.sigmoid(self.linear_z(combined))\n",
    "        \n",
    "        h_combine = torch.cat((x_t,r_t * h_prev),dim=1)\n",
    "        h_candidate_t = torch.tanh(self.linear_h(h_combine))\n",
    "        \n",
    "        h_t = (1 - z_t) * h_prev + z_t * h_candidate_t\n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d70bb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B=64 , T=200 , input dimension=128, hidden dimemnsion=256\n",
      "[Standard GRU] Time taken: 0.099236 seconds\n",
      "[recoded GRU] Time taken: 0.074645 seconds\n",
      "input :  torch.Size([64, 200, 128])\n",
      "output :  torch.Size([64, 200, 256])\n",
      "Shapes matched.\n"
     ]
    }
   ],
   "source": [
    "# compare nn.GRU , hand coded GRU -> Cells\n",
    "gru_tests(nn.GRU,GRU_Cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7671f61",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "<center>\n",
    "<img src=\"assets/LSTMV.png\" width=\"80%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "995ced07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Cell(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim):\n",
    "        super(LSTM_Cell,self).__init__()\n",
    "        self.linear_f = nn.Linear(input_dim + hidden_dim , hidden_dim)\n",
    "        self.linear_i = nn.Linear(input_dim + hidden_dim , hidden_dim)\n",
    "        self.linear_c_tilda = nn.Linear(input_dim + hidden_dim , hidden_dim)\n",
    "        self.linear_o = nn.Linear(input_dim + hidden_dim , hidden_dim)\n",
    "        \n",
    "    def forward(self,x_t,h_prev,c_prev):\n",
    "        combined = torch.cat((x_t,h_prev),dim=1)\n",
    "        \n",
    "        f_t = torch.sigmoid(self.linear_f(combined))\n",
    "        i_t = torch.sigmoid(self.linear_i(combined))   \n",
    "        c_tilda_t = torch.sigmoid(self.linear_c_tilda(combined))\n",
    "        o_t = torch.sigmoid(self.linear_o(combined))\n",
    "        \n",
    "        c_t = f_t * (c_prev - 1) + i_t * c_tilda_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        \n",
    "        return h_t,c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687efa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B=64 , T=200 , input dimension=128, hidden dimemnsion=256\n",
      "[Standard LSTM] Time taken: 0.097613 seconds\n",
      "[recoded LSTM] Time taken: 0.101432 seconds\n",
      "input :  torch.Size([64, 200, 128])\n",
      "output :  torch.Size([64, 200, 256])\n",
      "Shapes matched.\n"
     ]
    }
   ],
   "source": [
    "# compare nn.LSTM , hand coded LSTM -> Cells\n",
    "lstm_tests(nn.LSTM,LSTM_Cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c91422",
   "metadata": {},
   "source": [
    "# Pseudocode: Min Version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0756d317",
   "metadata": {},
   "source": [
    "## minGRU: A Minimal GRU\n",
    "\n",
    "<center>\n",
    "<img src=\"assets/GRUV2.png\" width=\"80%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a5fcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minGRU\n",
    "class minGRU_Cell(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim):\n",
    "        super(minGRU_Cell, self).__init__()\n",
    "        \n",
    "        self.linear_z = nn.Linear(input_dim,hidden_dim)\n",
    "        self.linear_h = nn.Linear(input_dim,hidden_dim)\n",
    "        \n",
    "    def forward(self,x_t,h_prev):\n",
    "        \n",
    "        z_t = torch.sigmoid(self.linear_z(x_t))\n",
    "        h_tilda_t = self.linear_h(x_t)\n",
    "        h_t = (1 - z_t) * h_prev + z_t * h_tilda_t\n",
    "        return h_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a661ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B=64 , T=200 , input dimension=128, hidden dimemnsion=256\n",
      "[Standard GRU] Time taken: 0.067931 seconds\n",
      "[recoded GRU] Time taken: 0.030765 seconds\n",
      "input :  torch.Size([64, 200, 128])\n",
      "output :  torch.Size([64, 200, 256])\n",
      "Shapes matched.\n"
     ]
    }
   ],
   "source": [
    "# compare nn.GRU , MinGRU -> Cells\n",
    "gru_tests(nn.GRU,minGRU_Cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95288021",
   "metadata": {},
   "source": [
    "## Implementation with log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a492419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return torch.where(x >= 0, x + 0.5, x.sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5985c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minGRU_log\n",
    "class log_minGRU_Cell(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim):\n",
    "        super(log_minGRU_Cell, self).__init__()\n",
    "        \n",
    "        self.linear_z = nn.Linear(input_dim,hidden_dim)\n",
    "        self.linear_h = nn.Linear(input_dim,hidden_dim)\n",
    "        \n",
    "    def forward(self,x_t,h_prev):\n",
    "        \n",
    "        z_t = torch.sigmoid(self.linear_z(x_t))\n",
    "        h_tilda_t = g(self.linear_h(x_t))\n",
    "        h_t = (1 - z_t) * h_prev + z_t * h_tilda_t\n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7afc31d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B=64 , T=200 , input dimension=128, hidden dimemnsion=256\n",
      "[Standard GRU] Time taken: 0.057156 seconds\n",
      "[recoded GRU] Time taken: 0.053628 seconds\n",
      "input :  torch.Size([64, 200, 128])\n",
      "output :  torch.Size([64, 200, 256])\n",
      "Shapes matched.\n"
     ]
    }
   ],
   "source": [
    "# compare nn.GRU , log-space minGRU -> Cells\n",
    "gru_tests(nn.GRU,log_minGRU_Cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede0d16d",
   "metadata": {},
   "source": [
    "## Parallel implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f6fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel \n",
    "def log_g(x):\n",
    "    return torch.where(x >= 0,(x + 0.5).log(),-F.softplus(-x))\n",
    "\n",
    "\n",
    "def parallel_scan_log(log_coeffs, log_values):\n",
    "    # log_coeffs: (batch_size, seq_len, input_size)\n",
    "    # log_values: (batch_size, seq_len + 1, input_size)\n",
    "    a_star = F.pad(torch.cumsum(log_coeffs, dim=1), (0, 0, 1, 0))\n",
    "    log_h0_plus_b_star = torch.logcumsumexp(\n",
    "    log_values - a_star, dim=1)\n",
    "    log_h = a_star + log_h0_plus_b_star\n",
    "    return torch.exp(log_h)[:, 1:]\n",
    "\n",
    "# minGRU_log parallel\n",
    "class parallel_log_minGRU(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim):\n",
    "        super(parallel_log_minGRU, self).__init__()\n",
    "        \n",
    "        self.linear_z = nn.Linear(input_dim,hidden_dim)\n",
    "        self.linear_h = nn.Linear(input_dim,hidden_dim)    \n",
    "    \n",
    "    def forward(self,x,h_prev):\n",
    "        \n",
    "        log_z = -F.softplus(-self.linear_z(x))\n",
    "        log_coeffs = -F.softplus(self.linear_z(x))\n",
    "        log_h = log_g(h_prev).unsqueeze(1)\n",
    "        log_h_tilda = log_g(self.linear_h(x))\n",
    "        h_t = parallel_scan_log(log_coeffs,torch.cat([log_h,log_z + log_h_tilda],dim=1))\n",
    "        return h_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715bb916",
   "metadata": {},
   "source": [
    "## minLSTM: A Minimal LSTM\n",
    "\n",
    "<center>\n",
    "<img src=\"assets/LSTMV2.png\" width=\"80%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f185d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minLSTM\n",
    "class minLSTM_Cell(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(minLSTM_Cell,self).__init__()\n",
    "        \n",
    "        self.linear_f = nn.Linear(input_dim,output_dim)\n",
    "        self.linear_i = nn.Linear(input_dim,output_dim)\n",
    "        self.linear_h = nn.Linear(input_dim,output_dim)\n",
    "        \n",
    "    def forward(self,x_t,h_0):\n",
    "        f_t = torch.sigmoid(self.linear_f(x_t))\n",
    "        i_t = torch.sigmoid(self.linear_i(x_t))\n",
    "        \n",
    "        h_tilda_t = self.linear_h(x_t)\n",
    "        \n",
    "        f_prime_t = f_t / (f_t + i_t)\n",
    "        i_prime_t = i_t / (f_t + i_t)\n",
    "        \n",
    "        h_t = f_prime_t * h_0 + i_prime_t * h_tilda_t\n",
    "        \n",
    "        return h_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a45830",
   "metadata": {},
   "source": [
    "## log implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6420e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_space minLSTM\n",
    "class log_minLSTM_Cell(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(log_minLSTM_Cell,self).__init__()\n",
    "        \n",
    "        self.linear_f = nn.Linear(input_dim,output_dim)\n",
    "        self.linear_i = nn.Linear(input_dim,output_dim)\n",
    "        self.linear_h = nn.Linear(input_dim,output_dim)\n",
    "        \n",
    "    def forward(self,x_t,h_0):\n",
    "        f_t = torch.sigmoid(self.linear_f(x_t))\n",
    "        i_t = torch.sigmoid(self.linear_i(x_t))\n",
    "        \n",
    "        h_tilda_t = g(self.linear_h(x_t))\n",
    "        \n",
    "        f_prime_t = f_t / (f_t + i_t)\n",
    "        i_prime_t = i_t / (f_t + i_t)\n",
    "        \n",
    "        h_t = f_prime_t * h_0 + i_prime_t * h_tilda_t\n",
    "        \n",
    "        return h_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1700fd61",
   "metadata": {},
   "source": [
    "## parallel implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fcd1121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_space minLSTM\n",
    "class parallel_log_minLSTM(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(parallel_log_minLSTM,self).__init__()\n",
    "        \n",
    "        self.linear_f = nn.Linear(input_dim,output_dim)\n",
    "        self.linear_i = nn.Linear(input_dim,output_dim)\n",
    "        self.linear_h = nn.Linear(input_dim,output_dim)\n",
    "        \n",
    "    def forward(self,x,h_0):\n",
    "        diff = F.softplus(-self.linear_f(x)) / -F.softplus(-self.linear_i(x))\n",
    "        log_f = -F.softplus(diff)\n",
    "        log_i = -F.softplus(-diff)\n",
    "        log_h_0 = torch.log(h_0).unsqueeze(1)\n",
    "        log_tilde_h = log_g(self.linear_h(x))\n",
    "        h = parallel_scan_log(log_f,torch.cat([log_h_0, log_i + log_tilde_h], dim=1))\n",
    "        return h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "146cb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, input_dim, hidden_dim = 64,200, 128,256\n",
    "\n",
    "# Random sequence input (batch_size, seq_len, input_dim)\n",
    "x = torch.randn(B,T, input_dim)\n",
    "# Initial hidden state (batch_size, hidden_dim)\n",
    "h0 = torch.randn(B, hidden_dim)\n",
    "\n",
    "# Initialize model\n",
    "parallel_log_minLSTM = parallel_log_minLSTM(input_dim, hidden_dim)\n",
    "log_minLSTM = log_minLSTM_Cell(input_dim, hidden_dim)\n",
    "\n",
    "\n",
    "p_l_out = parallel_log_minLSTM(x,h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b023e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h0\n",
    "outputs = []\n",
    "for t in range(T):\n",
    "    h = log_minLSTM(x[:, t, :], h)\n",
    "    outputs.append(h.unsqueeze(1))\n",
    "l_out = torch.cat(outputs, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
